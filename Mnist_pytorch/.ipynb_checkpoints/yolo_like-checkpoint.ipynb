{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "import math\n",
    "#load the data\n",
    "pkl_file = open('two_numbers_dataset_6k.pkl', 'rb')\n",
    "raw_datasets = pickle.load(pkl_file, encoding = 'bytes')\n",
    "img = []\n",
    "label = []\n",
    "location = []\n",
    "batch_size = 20\n",
    "for i in range(len(raw_datasets)):\n",
    "    img1 = transforms.ToTensor()(raw_datasets[i][0].reshape(80, 80, 1))\n",
    "    img.append(img1)\n",
    "    label.append([raw_datasets[i][1][0][0], raw_datasets[i][1][1][0]])\n",
    "    location.append([raw_datasets[i][1][0][1:], raw_datasets[i][1][1][1:]])\n",
    "train_len = int(2 * len(raw_datasets)/3)\n",
    "train_img = img[:train_len]\n",
    "test_img = img[train_len:len(raw_datasets)]\n",
    "tr_list_label = label[:train_len]\n",
    "te_list_label = label[train_len:len(raw_datasets)]\n",
    "tr_list_location = location[:train_len]\n",
    "te_list_location = location[train_len:len(raw_datasets)]\n",
    "train_label = []\n",
    "test_label = []\n",
    "train_location = []\n",
    "test_location = []\n",
    "for i in range(train_len):\n",
    "    train_label.append(torch.from_numpy(np.array(tr_list_label[i])))\n",
    "    train_location.append(torch.from_numpy(np.array(tr_list_location[i])))\n",
    "for i in range(len(raw_datasets) - train_len):\n",
    "    test_label.append(torch.from_numpy(np.array(te_list_label[i])))\n",
    "    test_location.append(torch.from_numpy(np.array(te_list_location[i])))\n",
    "batch_img = []\n",
    "batch_label = []\n",
    "batch_location = []\n",
    "for i in range(int(train_len/batch_size)):\n",
    "    batch_img.append(torch.stack(train_img[batch_size * i : batch_size * (i+1)], 0))\n",
    "    batch_label.append(torch.stack(train_label[batch_size * i : batch_size * (i+1)], 0))\n",
    "    batch_location.append(torch.stack(train_location[batch_size * i : batch_size * (i + 1)], 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Module\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class yolo_like(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(yolo_like, self).__init__()\n",
    "        self.yolo = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding = 1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding = 1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding = 1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding = 1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128,256, 3, padding = 1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding = 1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(256,512, 3, padding = 1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding = 1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(2)  # 5 * 5 * 512\n",
    "        )\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = nn.Linear(5 * 5 * 512, 1024)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(1024, 5 * 5 * (1 + 2))\n",
    "    def forward(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        output = self.yolo(x)\n",
    "        output = self.flatten(output)\n",
    "        output = F.leaky_relu(self.fc1(output), 0.1)\n",
    "        output = self.fc2(output)\n",
    "        return output  #linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(loss, self).__init__()\n",
    "        return\n",
    "    def forward(self, pred, truth, S = 5):\n",
    "        \"\"\"args : pred : (batch, S * S * 3)\n",
    "                  truth: (batch, n, [x, y])\"\"\"\n",
    "        batch_size = pred.size(0)\n",
    "        obj_num = truth.size(1)\n",
    "        #ground truth\n",
    "        #extract the coordinate prediction\n",
    "        pred = pred.data\n",
    "        truth = truth.data\n",
    "        truth = truth.float()\n",
    "        prediction = pred.contiguous().view(-1, S * S, 3)\n",
    "        pred_coord = prediction[:, :, 1:3].contiguous()\n",
    "        pred_conf = prediction[:, :, 0].contiguous()\n",
    "        truth_posi = truth[:, :, 0:2]\n",
    "        pred_coord1 = torch.zeros(batch_size, S * S, 2)\n",
    "        # normalize to make it fall on 0-1\n",
    "        truth_posi_x = ((truth[:, :, 0] + 14) % (80 / S)) * (S / 80) #batch_size * obj_num\n",
    "        truth_posi_y = ((truth[:, :, 1] + 14) % (80 / S)) * (S / 80)\n",
    "        label_ceil = ((truth[:, :, 0] + 14 - (truth[:, :, 0] + 14) % (80 / S)) / (80 / S)) * S + (truth[:, :, 1] + 14 - (truth[:, :, 1] + 14) % (80 / S)) / (80 / S)\n",
    "        p_loss = torch.zeros(batch_size, S * S, 1).cuda()\n",
    "        #Confidence = P * IOU \n",
    "        for i in range(batch_size):\n",
    "            for j in range(obj_num):\n",
    "                p_loss[i, int(label_ceil[i, j]), 0] = math.pow(prediction[i, int(label_ceil[i, j]), 0] - 1 * IOU(pred_coord[i, int(label_ceil[i, j]), 0], pred_coord[i, int(label_ceil[i, j]), 1], truth_posi_x[i, j], truth_posi_y[i, j]), 2)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(S * S):\n",
    "                for x in range(obj_num):\n",
    "                    if j != label_ceil[i, x]:\n",
    "                        p_loss[i, j, 0] =  0.5 * math.pow(prediction[i, j, 0] - 0.0, 2)\n",
    "        pred_x = pred_coord[:, :, 0]\n",
    "        pred_y = pred_coord[:, :, 1] # batch_size * SS\n",
    "        dis_loss = torch.zeros(batch_size, S * S).cuda()\n",
    "        for i in range(batch_size):\n",
    "            for j in range(obj_num):\n",
    "                dis_loss[i, j] = math.pow(pred_x[i, int(label_ceil[i, j])] - truth_posi_x[i, j], 2) + math.pow(pred_y[i, int(label_ceil[i, j])] - truth_posi_y[i, j], 2)\n",
    "        dis_loss = 5 * dis_loss\n",
    "        loss1 = torch.mean(dis_loss + p_loss, 1)\n",
    "        loss2 = torch.mean(loss1, 0)\n",
    "        return (Variable(loss2, requires_grad = True))\n",
    "#for data, target in zip(batch_img, batch_location):\n",
    "    #data, target = Variable(data), Variable(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def IOU(pred_coord_x, pred_coord_y, truth_position_x, truth_position_y):\n",
    "    #pred_coord is 2, x, y is relative to the grid\n",
    "    #x is a num and y is a num relative to grid\n",
    "    x1 = min((28 - 2 * abs(pred_coord_x * 16 - truth_position_x * 16)), 0)\n",
    "    y1 = min((28 - 2 * abs(pred_coord_y * 16 - truth_position_y * 16)), 0)\n",
    "    x2 = 28 + abs(pred_coord_x * 16 - truth_position_x * 16)\n",
    "    y2 = 28 + abs(pred_coord_y * 16 - truth_position_y * 16)\n",
    "    return (x1 * y1)/(x2 * y2)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/torch/tensor.py:293: UserWarning: self and other not broadcastable, but have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  return self.add(other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 0\tLoss: 0.224\n",
      "Train Step: 80\tLoss: 0.233\n",
      "Train Step: 160\tLoss: 0.259\n",
      "Train Step: 240\tLoss: 0.264\n",
      "Train Step: 320\tLoss: 0.276\n",
      "Train Step: 400\tLoss: 0.224\n",
      "Train Step: 480\tLoss: 0.233\n",
      "Train Step: 560\tLoss: 0.259\n",
      "Train Step: 640\tLoss: 0.264\n",
      "Train Step: 720\tLoss: 0.276\n",
      "Train Step: 800\tLoss: 0.224\n",
      "Train Step: 880\tLoss: 0.233\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-fbe08589e9d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mave_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mave_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mave_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-bde4afe4ddeb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pred, truth, S)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtruth_posi_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlabel_ceil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m14\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtruth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtruth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m14\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtruth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m#Confidence = P * IOU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = yolo_like()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01 ,weight_decay=5e-4)\n",
    "model.train()\n",
    "train_loss = []\n",
    "train_accu = []\n",
    "i = 0\n",
    "criterion = loss()\n",
    "for epoch in range(10):\n",
    "    for data, target in zip(batch_img, batch_location):\n",
    "        optimizer.zero_grad()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "        output = model(data)\n",
    "        ave_loss = criterion(output, target)\n",
    "        ave_loss.backward()\n",
    "        train_loss.append(ave_loss.data[0])\n",
    "        optimizer.step()\n",
    "        if i % 80 == 0:\n",
    "            print('Train Step: {}\\tLoss: {:.3f}'.format(i, ave_loss.data[0]))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of range for dimension 0 (of size 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-43209c31cada>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mnon_max_supp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-43209c31cada>\u001b[0m in \u001b[0;36mnon_max_supp\u001b[0;34m(output, S, threshold)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpro_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpro_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpro_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mpro_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpro_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbox_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpro_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of range for dimension 0 (of size 1)"
     ]
    }
   ],
   "source": [
    "#performing non_max_suppression\n",
    "def non_max_supp(output, S = 5, threshold = 0.1): #(S * S, 3)\n",
    "    batch_size = output.size(0)\n",
    "    output = output.data\n",
    "    pro_output = output.contiguous().view(S * S, 3)\n",
    "    for j in range(S * S):\n",
    "        pro_output[j, 1] = pro_output[j, 1] * 16 + int((j+1) / 5) * 16\n",
    "        pro_output[j, 2] = pro_output[j, 2] * 16 + (j - 5 * int((j+1) / 5)) * 16\n",
    "    box_score = pro_output[:, 0]\n",
    "    box_x = pro_output[:, 1]\n",
    "    box_y = pro_output[:, 2]\n",
    "    filter_mask = (box_score >= threshold)\n",
    "    box_score = box_score[filter_mask]\n",
    "    box_x = box_score[filter_mask]\n",
    "    box_y = box_score[filter_mask]\n",
    "    select_location = []\n",
    "    wait_location = []\n",
    "    for j in range(10):\n",
    "        max_score_index = torch.max(box_score, 0)[1][0]\n",
    "        max_score = torch.max(box_score, 0)[0][0]\n",
    "        box_x_max = box_x[max_score_index]\n",
    "        box_y_max = box_y[max_score_index]\n",
    "        selection_location.append([max_score, box_x_max, box_y_max])\n",
    "        for i in range(len(box_score)):\n",
    "            if IOU(box_x_max, box_y_max, box_x[i], box_y[i]) <= 0.4:\n",
    "                wait_location.append(i)\n",
    "        box_score = torch.take(box_score, torch.LongTensor(wait_location))\n",
    "        box_x = torch.take(box_x, torch.LongTensor(wait_location))\n",
    "        box_y = torch.take(box_y, torch.LongTensor(wait_location))\n",
    "    return select_location\n",
    "\n",
    "x = Variable(torch.randn(25, 3))\n",
    "non_max_supp(x)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      "[torch.ByteTensor of size 4]\n",
      "\n",
      "\n",
      " 0.5897\n",
      " 0.8553\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "\n",
      " 0.3587\n",
      " 0.6134\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-ca6bedfd1fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: len() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "def picture_crop(select_location):\n",
    "    for i in range(len(select_location)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
